{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vAWGMwiT3qC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "<h1><div style=\"text-align: center;\"> COVID-19 Detection and Instance semantic\n",
    "segmentation</div></h1>\n",
    "\n",
    "<h3><div style=\"text-align: center;\"> A Ascencio-Cabral\n",
    "</div></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpAmWYRYw85G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 Dataset - Merged Dataset\n",
    "\n",
    "\n",
    "The 3D data was ingested from four public sources:\n",
    "\n",
    "- COVID-19 Lung CT Lesion Segmentation - Grand Challenge 2020, 199 patients\n",
    "https://covid-segmentation.grand-challenge.org/COVID-19-20/\n",
    "\n",
    "- COVID-19 CT Lung and Infection Segmentation Dataset Zenodo - 20 patients (only infection mask\n",
    "used)\n",
    "https://zenodo.org/record/3757476#.YTdEx55Kg1h\n",
    "\n",
    "- Italian Society of Medical and Interventional Radiology(ISIRM), a volume with images from 48 patients -  Dataset available on SegMed website https://medicalsegmentation.com/covid19/\n",
    "\n",
    "-  COVID-19 CT scan lesion segmentation dataset â€“ Only the sliced CT and Masks from the MosMedData Dataset COVID19_1110 sliced were used.\n",
    "https://www.kaggle.com/datasets/maedemaftouni/covid19-ct-scan-lesion-segmentation-dataset\n",
    "\n",
    "Except for the images from the Morozov dataset, all the other datasets were compressed NIfTi.\n",
    "Volumes (nii.gz) were sliced on plane z and converted to 2D png images. The images were\n",
    "process using\n",
    "\n",
    "The images were merged into one dataset.\n",
    "\n",
    "\n",
    "### 1.2 Models\n",
    "\n",
    "- **Mask-RCNN-50-FPN and customised versions**\n",
    "- **Mask-RCNN-MobileNet-v3-large-FPN**\n",
    "\n",
    "The model used to segment COVID-19 lesions is Mask-RCNN. This model is the base to build our classification models. Our model consists of Mask-RCNN with two feature extractors, ResNet-50-FPN and MobileNet-v3-large-FPN. Mask-RCNN-ResNet-50-FPN is an off-the-box model from the PyTorch Framework. The anchor generator of the network will be modified, and the kernel size of the first convolutional layer of the models' backbone will also be hacked. Mask-RCNN-MobileNet-v3-large-FPN is not an off-the-box network. We will also create this model with custom anchor generators in this work.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Exp |     Model Backbone     | Resized<br/> kernel | Custom <br/>Anchor Generator |\n",
    "|-----|:----------------------:|:-------------------:|:----------------------------:|\n",
    "| 1   |    ResNet-50-FPN-v2    |          -          |              -               |\n",
    "| 2   |    ResNet-50-FPN-v2    |          -          |             Yes              |\n",
    "| 3   |    ResNet-50-FPN-v2    |        3 x 3        |              -               |\n",
    "| 4   |    ResNet-50-FPN-v2    |        3 x 3        |             Yes              |\n",
    "| 5   |    ResNet-50-FPN-v2    |        5 x 5        |              -               |\n",
    "| 6   |    ResNet-50-FPN-v2    |        5 x 5        |             Yes              |\n",
    "| 7   | MobileNet-v3-large-FPN |         N/A         |              -               |\n",
    "\n",
    "</center>\n",
    "\n",
    "### **1.3 Approach to training - Transfer Learning**\n",
    "\n",
    "The approach to training the models is transfer learning by fine-tuning all layers of the deep neural networks.\n",
    "\n",
    "\n",
    "###  **1.4 Evaluation Metrics - Coco style**\n",
    "Evaluation Metrics - COCO style mean average precision mAP at different intersections over union (IoU) threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.5 Code Guidance\n",
    "\n",
    "The code implementation of this is adapted from Torch Vision Object Detection\n",
    "Tutorials:\n",
    "- [torchvision_tutorial-object-detection-finetuning-tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "- [oneoffcoder.com/object-detection](https://learn-pytorch.oneoffcoder.com/object-detection.html)\n",
    "- [pytorch/vision/tree/master/references/detection](https://github.com/pytorch/vision/tree/main/references/detection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comment out if pycocotools is installed\n",
    "# %%shell\n",
    "!pip install cython\n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%cd '/content/drive/MyDrive/covid_instance_seg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## __Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from detection import utils\n",
    "from detection import transforms as T\n",
    "from detection.engine import train_one_epoch, evaluate\n",
    "from utility.plotting import training_stats\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN, MaskRCNNPredictor, MaskRCNNHeads\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.mobilenetv3 import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "from torchvision.models.detection.backbone_utils import _validate_trainable_layers, \\\n",
    "    _mobilenet_extractor\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNConvFCHead\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from covid_dataset import CovidMerged\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(180)\n",
    "torch.manual_seed(180)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Accessing the dataset merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we will unzip the merged dataset. Comment out to decompress the images file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to unzip\n",
    "# !unzip './merged_dataset' -d './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Images and masks in dataset')\n",
    "print(len(os.listdir('merged_dataset/CT')),'&',\n",
    "      len(os.listdir('merged_dataset/GT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs = sorted(os.listdir('./merged_dataset/CT'))\n",
    "msks = sorted(os.listdir('./merged_dataset/GT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Dataset Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "covid_img = cv2.imread('merged_dataset/CT/ct_gz15z093.png')\n",
    "covid_img = cv2.cvtColor(covid_img, cv2.COLOR_RGBA2RGB)\n",
    "covid_mask =cv2.imread('merged_dataset/GT/gt_gz15z093.png')\n",
    "covid_mask = cv2.cvtColor(covid_mask, cv2.COLOR_RGBA2RGB)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(covid_img)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(covid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_gt= cv2.addWeighted(covid_img, 0.5, covid_mask, 0.5, 0)\n",
    "plt.imshow(image_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Building Mask-RCNN-50FPN  and Mask-RCNN-MobileNet- v3-fpn fuctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we will build the function to create the instance segmentation model with ResNet-50-FPN and MobileNet-v3-large-FPN backbones.\n",
    "PyTorch does not have an off-the-shelf method for Mask-RCNN with MobileNet-v3-FPN; we will use it as a base maskrcnn_resnet50_fpn function to buildmaskrcnn_mobilenet_v3_large_fpn.\n",
    "Additionally, we will build a function that utilises the Mask-RCNN-ResNet-50-fpn-v2 as the based\n",
    "model with the capability to customise the anchors and the kernel size of the first convolutional\n",
    "layer of the backbone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 __Build a customised anchor generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def anchorgen_rcnn():\n",
    "    \"\"\"\n",
    "    :return: AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "    \"\"\"\n",
    "    anchor_sizes = (8, 16, 32, 64, 128, 256)\n",
    "    aspect_ratios = (0.25, 0.5, 1.0, 1.5, 2.0)\n",
    "    return AnchorGenerator(sizes=tuple([anchor_sizes for _ in range(5)]),\n",
    "                           aspect_ratios=tuple([aspect_ratios for _ in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Building Mask-RCNN-50-FPN-V2 with customised anchors  and resized filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def customise_maskrcnn_50_fpn(num_classes=2, resized_conv=True, size=(5,5), custom_anchors=True):\n",
    "    \"\"\"\n",
    "    :param num_classes: number of classes to classify covid + 1 for the background\n",
    "    :param resized_conv: a boolean\n",
    "    :param size: a tuple with the dimesion to resize the kernel, the default is 5 x 5\n",
    "    :param custom_anchors: if customised anchors will be built\n",
    "    :return: maskrcnn-resnet-50-fpn with or without customed features\n",
    "    \"\"\"\n",
    "\n",
    "    assert size in [(3,3), (5,5), None], 'resize kernel to (3 x 3) or (5 x 5)'\n",
    "\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "\n",
    "    if resized_conv:\n",
    "        # The mask-rcnn standard kernel size is 7 x 7\n",
    "        model.backbone.body.conv1 = nn.Conv2d(3, 64, size, (2, 2), (3, 3), bias=False)\n",
    "\n",
    "    if custom_anchors:\n",
    "        new_anchors = anchorgen_rcnn()\n",
    "        model.rpn.anchor_generator = new_anchors\n",
    "\n",
    "        # Features that FPN returns\n",
    "        model.rpn.head = RPNHead(256, new_anchors.num_anchors_per_location()[0], conv_depth=2)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                        hidden_layer,\n",
    "                                                        num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Building Mask-RCNN-MobileNet-v3-FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def maskrcnn_mobilenet_v3_large_fpn(*,\n",
    "    num_classes=None, progress=True, weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V2,\n",
    "    trainable_backbone_layers=6):\n",
    "    \"\"\" \n",
    "    Class adapted from pytorch torchvision.models.detection.maskrcnn_resnet50_fpn` for more details.\n",
    "    https://github.com/pytorch/vision/blob/main/torchvision/models/detection/mask_rcnn.py\n",
    "    https://github.com/pytorch/vision/blob/70faba91d1c79f689ac6ebd30ad0c4be62690196\n",
    "    /torchvision/models/detection/faster_rcnn.py#L653\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of output classes of the model (including the background)\n",
    "        progress: boolean to display the weights downloading progress\n",
    "        weights_backbone (:class:`~torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V1`,): The\n",
    "            pretrained weights for the backbone.\n",
    "        trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from\n",
    "            final block. Valid values are between 0 and 6, with 6 meaning all backbone layers are\n",
    "            trainable. If ``None`` is passed (the default) this value is set to 3.\n",
    "    :returns Mask-RCNN model with MobileNet-v3-large-FPN backbone\n",
    "    \"\"\"\n",
    "\n",
    "    weights_backbone =  MobileNet_V3_Large_Weights.verify(weights_backbone)\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = 91\n",
    "\n",
    "    is_trained = weights_backbone is not None\n",
    "    trainable_backbone_layers = _validate_trainable_layers(is_trained, trainable_backbone_layers, 6, 3)\n",
    "    norm_layer = misc_nn_ops.FrozenBatchNorm2d if is_trained else nn.BatchNorm2d\n",
    "\n",
    "    backbone = mobilenet_v3_large(weights=weights_backbone, progress=progress, norm_layer=norm_layer)\n",
    "    backbone = _mobilenet_extractor(backbone, True, trainable_backbone_layers)\n",
    "    anchor_sizes = ((8, 16, 32, 64, 128, 256,),) * 3\n",
    "    aspect_ratios = ((0.25, 0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)   \n",
    "    rpn_head = RPNHead(backbone.out_channels, rpn_anchor_generator.num_anchors_per_location()[0], conv_depth=2)\n",
    "    box_head = FastRCNNConvFCHead(\n",
    "        (backbone.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=nn.BatchNorm2d)\n",
    "    # Normalisation can be 'None'\n",
    "    mask_head = MaskRCNNHeads(backbone.out_channels, [256, 256, 256, 256], 1, norm_layer=nn.BatchNorm2d)\n",
    "    model = MaskRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=rpn_anchor_generator,\n",
    "        rpn_score_thresh=0.05, # threshold from fasterrcnn_mobilenet_v3_large_fpn\n",
    "        rpn_head=rpn_head,\n",
    "        box_head=box_head,\n",
    "        mask_head=mask_head)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4 __Creating models__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(net_name, num_classes=2, resized_kernel=True, size=(5, 5), custom_anchors=True):\n",
    "    \"\"\"\n",
    "    :param net_name: a string with the name of the model to be built\n",
    "    :param num_classes: an integer with the number of classes + background\n",
    "    :param resized_kernel: a boolean indicating if the kermnel is resized or not, only \n",
    "    for models with resnet backbone\n",
    "    :param size: a tuple with the new kernel dimensions\n",
    "    :param custom_anchors: a boolean indicating whether to use the standard anchors or custom\n",
    "    anchors\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    assert net_name in ['maskrcnn-resnet50-fpn', 'maskrcnn-mobilenet-v3'], 'Input maskrcnn-resnet50-fpn or maskrcnn-mobilenet-v3'\n",
    "\n",
    "    if net_name == 'maskrcnn-resnet50-fpn':\n",
    "        model = customise_maskrcnn_50_fpn(num_classes=num_classes,\n",
    "                                          resized_conv=resized_kernel, size=size,\n",
    "                                          custom_anchors=custom_anchors)\n",
    "        return model\n",
    "\n",
    "    elif net_name == 'maskrcnn-mobilenet-v3':\n",
    "        model = maskrcnn_mobilenet_v3_large_fpn(num_classes=num_classes)\n",
    "        # print('......Maskrcnn-mobilenet-v3-large-fpn pretrained on ImageNet.....')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### __4.1 Transformations__\n",
    "\n",
    "Mask-RCNN class resizes the images. We will apply random horizontal flips and scale jitter\n",
    "augmentation during the training of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    trans = [T.PILToTensor(), T.ConvertImageDtype(dtype=torch.float32)]\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    if train:\n",
    "       # data augmentation during training\n",
    "        trans.extend([T.RandomHorizontalFlip(0.5), T.ScaleJitter((800, 800))])\n",
    "        # trans.append(T.FixedSizeCrop())\n",
    "    return T.Compose(trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### __4.2 Tracking time taken for training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def time_minutes(s):\n",
    "    \"\"\"\n",
    "    :param s:  time in seconds\n",
    "    :return: time in hh:mm:ss format\n",
    "    \"\"\"\n",
    "    return time.strftime('%H:%M:%S', time.gmtime(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### __4.3  Building the training function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset will be split  randomly in training, validation and test subsets. First a list of\n",
    "random indices will be generated by using a random permutation. Then split of rations are\n",
    "80/10/10 for training, validation and test respectively.\n",
    "\n",
    "To build the main training function we will use train_one_epoch\n",
    "and evaluate functions from ```engine.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main(**train_params):\n",
    "    \"\"\"\n",
    "    :param train_params:  aregument containing the training hyperparameters\n",
    "    :return: the training  metrics, indices to  prepare the testdataset, the name of the model\n",
    "    trained, and the time taken for training.\n",
    "    \"\"\"\n",
    "\n",
    "    assert train_params['scheduler'] in ['plateau', 'steps'], 'Only ReduceLROnPlateau ' \\\n",
    "                                                              'and StepLR schedulers available'\n",
    "\n",
    "    train_path = os.path.join(train_params['data_dir'])\n",
    "\n",
    "    dataset = CovidMerged(train_path, min_area=train_params['min_area'],\n",
    "                          trans=get_transform(train=True))\n",
    "    val_dataset = CovidMerged(train_path, min_area=train_params['min_area'],\n",
    "                              trans=get_transform(train=False))\n",
    "\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    indx = round(len(dataset) * 0.80)\n",
    "    indx_ = round(len(dataset) * 0.90)\n",
    "    train_dataset = torch.utils.data.Subset(dataset, indices[0:indx])\n",
    "    val_dataset = torch.utils.data.Subset(val_dataset, indices[indx:indx_])\n",
    "    test_idx = indices[indx_:]\n",
    "\n",
    "    # Training and validation data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], shuffle=True,\n",
    "                              num_workers=train_params['workers'], collate_fn=utils.collate_fn)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_params['batch_size'], shuffle=False,\n",
    "                            num_workers=train_params['workers'], collate_fn=utils.collate_fn)\n",
    "\n",
    "    # Make the model using the helper function\n",
    "    model = get_model(train_params['model'], train_params['num_classes'],\n",
    "                      resized_kernel=train_params['resize'], size=train_params['size'],\n",
    "                      custom_anchors=train_params['anchors'])\n",
    "\n",
    "    # send model to the available device GPU or CPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # construct the optimizer\n",
    "    param = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    if train_params['optimizer'] == 'SGD':\n",
    "        optimizer = torch.optim.SGD(param, lr=train_params['lr'], momentum=train_params['momentum'],\n",
    "                                    weight_decay=train_params['w_decay'],\n",
    "                                    nesterov=train_params['nesterov'])\n",
    "\n",
    "    elif train_params['optimizer'] == 'Adam':\n",
    "        optimizer = torch.optim.Adam(param, lr=train_params['lr'],\n",
    "                                     weight_decay=train_params['w_decay'])\n",
    "\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(param, lr=train_params['lr'])\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    if train_params['scheduler'] == 'steps':\n",
    "        assert train_params['steps'] is not None,  'Input the number of steps'\n",
    "        print('steps scheduler')\n",
    "        lr_scheduler = StepLR(optimizer, step_size=train_params['steps'],\n",
    "                              gamma=train_params['gamma'])\n",
    "\n",
    "    else:\n",
    "        print('plateau scheduler')\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=train_params['gamma'],\n",
    "                                         patience=1)\n",
    "\n",
    "    net_name = train_params['model']\n",
    "    ex = train_params['exp']\n",
    "\n",
    "    if train_params['resize'] and train_params['anchors']:\n",
    "        net_name = f'exp_{ex}_{net_name}_ra'\n",
    "    elif train_params['resize'] and not train_params['anchors']:\n",
    "        net_name = f'exp_{ex}_{net_name}_r'\n",
    "    elif not train_params['resize'] and train_params['anchors']:\n",
    "        net_name = f'exp_{ex}_{net_name}_a'\n",
    "    else:\n",
    "        net_name = f'exp_{ex}_{net_name}'\n",
    "\n",
    "    if train_params['resume_training']:\n",
    "        weights_dir = f\"{train_params['output_dir']}/{net_name}_resume.pth\"\n",
    "        check_point = torch.load(weights_dir, map_location='cpu')\n",
    "        model.load_state_dict(check_point['model'])\n",
    "        optimizer.load_state_dict(check_point['optimizer_dict'])\n",
    "        s_epoch = check_point['epoch']\n",
    "        lr_scheduler.load_state_dict(check_point['scheduler'])\n",
    "        stats_log = check_point['stats']\n",
    "        best_mAPs = np.max(stats_log['mAPseg'])\n",
    "        print('resume')\n",
    "\n",
    "    else:\n",
    "        s_epoch = 0\n",
    "        best_mAPs = 0.\n",
    "        stats_log = defaultdict(list)\n",
    "\n",
    "    print(f'Training {net_name}')\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for epoch in range(s_epoch, train_params['num_epochs']):\n",
    "\n",
    "        # train one epoch\n",
    "        loss, _ = train_one_epoch(model, optimizer, train_loader, device, epoch,\n",
    "                                  print_freq=200)\n",
    "\n",
    "        # evaluate on the validation dataset\n",
    "        _, stats = evaluate(model, val_loader, device)\n",
    "\n",
    "        # AP @ IoU=0.5 and IoU= 0:50:0:95 for bboxes\n",
    "        mAPb, mAPb50 = round(stats['bbox'][0], 4), round(stats['bbox'][1], 4)\n",
    "\n",
    "        # AP @ IoU=0.5 and IoU= 0:50:0:95 for segmentation\n",
    "        mAPseg, mAP50seg = round(stats['segm'][0], 4), round(stats['segm'][1], 4)\n",
    "\n",
    "        # AP @ IoU=0.75 for detections and segmentations\n",
    "        mAPb75, mAP75seg = round(stats['bbox'][2], 4), round(stats['segm'][2], 4)\n",
    "\n",
    "        # update the learning rate\n",
    "        if train_params['scheduler'] == 'plateau':\n",
    "            lr_scheduler.step(mAPseg)\n",
    "\n",
    "        else:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        stats_log['mAPb'].append(mAPb)\n",
    "        stats_log['mAPseg'].append(mAPseg)\n",
    "        stats_log['mAPb50'].append(mAPb50)\n",
    "        stats_log['mAP50seg'].append(mAP50seg)\n",
    "        stats_log['mAPb75'].append(mAPb75)\n",
    "        stats_log['mAP75seg'].append(mAP75seg)\n",
    "        stats_log['train_loss'].append(loss)\n",
    "\n",
    "        if mAPseg > best_mAPs:\n",
    "            best_mAPs = mAPseg\n",
    "            # Save the best weights for test\n",
    "            check_point = {'exp': ex, 'model': model.state_dict(),\n",
    "                           'epoch': epoch + 1,\n",
    "                           'optimizer_dict': optimizer.state_dict(),\n",
    "                           'lr': train_params['lr'], 'scheduler': lr_scheduler.state_dict(),\n",
    "                           'test_idx': test_idx}\n",
    "\n",
    "            torch.save(check_point, os.path.join(train_params['output_dir'],\n",
    "                                                 f'{net_name}.pth'))\n",
    "\n",
    "        # Save weights to continue training if interrupted\n",
    "        torch.save({'exp': ex, 'model': model.state_dict(), 'epoch': epoch + 1,\n",
    "                    'optimizer_dict': optimizer.state_dict(),\n",
    "                    'lr': train_params['lr'], 'scheduler': lr_scheduler.state_dict(),\n",
    "                    'stats': stats_log, 'test_idx': test_idx},\n",
    "                   os.path.join(train_params['output_dir'], f'{net_name}_resume.pth'))\n",
    "\n",
    "    total_time = time.time() - epoch_start_time\n",
    "    print('Training time {}'.format(time_minutes(total_time)))\n",
    "    print('Best val_mAPseg: {0:.4f}'.format(best_mAPs))\n",
    "\n",
    "    return stats_log, net_name, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>5. Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exps =[i for i in range(1, 7)]\n",
    "exp_models = sorted(['maskrcnn-resnet50-fpn'] * 6)\n",
    "resized = [False, False] + [True] * 4\n",
    "kernel_sizes = [None, None, (3, 3), (3,  3), (5, 5), (5,  5)]\n",
    "custom_anchor = [False, True] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "setup = pd.DataFrame({'Exp':exps, 'Net': exp_models,  'resized': resized, 'sizes': kernel_sizes,\n",
    "                      'custom_anchors': custom_anchor })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "setup.loc[len(setup.index)] = [7, 'maskrcnn-mobilenet-v3', False , False, False]\n",
    "setup.set_index('Exp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "<h3> <center> Experimental Design </center></h3>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Exp |               Net                | Resized </br>Kernel | Sizes | Customed </br>Anchors |\n",
    "|-----|:--------------------------------:|:-------------------:|:-----:|:---------------------:|\n",
    "| 1   |     Mask-RCNN-ResNet-50-FPN      |          -          |   -   |           -           |\n",
    "| 2   |     Mask-RCNN-ResNet-50-FPN      |          -          |   -   |         True          |\n",
    "| 3   |     Mask-RCNN-ResNet-50-FPN      |        True         | 3 x 3 |           -           |\n",
    "| 4   |     Mask-RCNN-ResNet-50-FPN      |        True         | 3 x 3 |         True          |\n",
    "| 5   |     Mask-RCNN-ResNet-50-FPN      |        True         | 5 x 5 |           -           |\n",
    "| 6   |     Mask-RCNN-ResNet-50-FPN      |        True         | 5 x 5 |         True          |\n",
    "| 7   | Mask-RCNN-MobileNet-v3-large-FPN |          -          |   -   |           -           |\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### 5.2 Hyper-parameters selection and training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'merged_dataset')\n",
    "out_dir = os.path.join(os.getcwd(), 'weights')\n",
    "\n",
    "try:\n",
    "    os.makedirs(out_dir, exist_ok=False)\n",
    "    print('Directory successfully created')\n",
    "except OSError as error:\n",
    "    print('Directory already exist')\n",
    "\n",
    "# Extract the model configuration from the experimental setup dataframe\n",
    "exp = 7\n",
    "set_model = setup.loc[exp]['Net']\n",
    "setup_resize = setup.loc[exp]['resized']\n",
    "setup_sizes = setup.loc[exp]['sizes']\n",
    "setup_anchors = setup.loc[exp]['custom_anchors']\n",
    "\n",
    "schedulers = ['steps', 'plateau']\n",
    "\n",
    "resume = False\n",
    "\n",
    "params= {'exp': exp, 'model': set_model, 'num_classes': 2, 'resize': setup_resize,\n",
    "         'size': setup_sizes, 'anchors': setup_anchors, 'data_dir' : data_dir,\n",
    "         'output_dir' : out_dir, 'min_area': 10, 'batch_size': 4, 'lr' : 0.000015,\n",
    "         'optimizer': 'AdamW', 'momentum':None, 'w_decay' : None, 'nesterov': False,\n",
    "         'scheduler': schedulers[1], 'steps': None, 'gamma': 0.8,\n",
    "         'num_epochs': 50, 'resume_training': resume, 'workers': 2}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_metrics, model_name, train_time = main(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the two lines below only if run out of CUDA memory and re-run the cell above,\n",
    "# alternatyve re-start the whole runtime and run the notebook\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2> 6. Results Visualisation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the directory for the plots and images\n",
    "figs_dir = os.path.join(os.getcwd(), 'figures')\n",
    "try:\n",
    "    os.makedirs(figs_dir, exist_ok=False)\n",
    "    print('Directory successfully created')\n",
    "except OSError as error:\n",
    "    print('Directory already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the best weights for testing\n",
    "checkpoint_dir = os.path.join(out_dir, f'{model_name}.pth')\n",
    "checkpoint = torch.load(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "training_stats(training_metrics, model_name, figs_dir=figs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2> 7. Evaluating the model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 7.1 Create and process the test dataset and create the dataloaders </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = params['data_dir']\n",
    "test_indices = checkpoint['test_idx']\n",
    "covid = CovidMerged(dataset_path, min_area=params['min_area'], trans=get_transform(train=False))\n",
    "test_dataset = torch.utils.data.Subset(covid, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4,\n",
    "                    shuffle=False, num_workers=2, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> 7.2 Load the best training weights onto the model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_ = get_model(params['model'], params['num_classes'], params['resize'], params['size'],\n",
    "                   params['anchors'])\n",
    "\n",
    "model_.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ , test_stats = evaluate(model_.to(device), test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>7.3 Save metrics and  training setup </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Rounding the output of the metrics to 4 figures\n",
    "for k,v in test_stats.items():\n",
    "    test_stats[k] = np.round(v, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_dir = os.path.join(os.getcwd(), 'predictions')\n",
    "try:\n",
    "    os.makedirs(pred_dir, exist_ok=False)\n",
    "    print('Directory successfully created')\n",
    "except OSError as error:\n",
    "    print('Directory already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract metrics from coco evaluator 0 is the overall range, 1 indicates IoU @0.50 and 2 IoU @0.75\n",
    "metrics = {'AP bbox': test_stats['bbox'][0], 'AP bbox @0.50': test_stats['bbox'][1],\n",
    "           'AP bbox @0.75': test_stats['bbox'][2], 'AP seg': test_stats['segm'][0],\n",
    "           'AP seg @ 0.50' : test_stats['segm'][1], 'AP seg @0.75': test_stats['segm'][2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(metrics, index =[0])\n",
    "df_metrics.insert(0, 'Exp', params['exp'])\n",
    "df_metrics.insert(1, 'Date', datetime.date.today())\n",
    "df_metrics.insert(2, 'Model', model_name)\n",
    "df_metrics.insert(3, 'Epochs', params['num_epochs'])\n",
    "df_metrics.insert(4, 'Optimizer', params['optimizer'])\n",
    "df_metrics.insert(5, 'Learning rate', params['lr'])\n",
    "df_metrics.insert(6, 'Scheduler', params['scheduler'])\n",
    "df_metrics.insert(7, 'Steps', params['steps'])\n",
    "df_metrics.insert(8, 'Gamma', params['gamma'])\n",
    "df_metrics.insert(9, 'Best epoch', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = params['num_epochs']\n",
    "df_metrics.to_csv(os.path.join(pred_dir,  f'{model_name}_{epochs}.csv'))\n",
    "df_metrics.set_index('Exp', inplace=True)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>8. Inference </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>8.1 Inference functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coloured_mask(mask, colour='blue'):\n",
    "    \"\"\"\n",
    "    :param colour: str one of the colors blue, cyan and pink\n",
    "    :param mask: a numpy array of the masks\n",
    "    :return: the colour mask and the colour selected\n",
    "    \"\"\"\n",
    "    # One color for all COVID instances\n",
    "    if colour == 'blue':\n",
    "        colour = [0, 0, 255]\n",
    "    elif colour == 'cyan':\n",
    "        colour = [0, 255, 255]\n",
    "    elif colour == 'green':\n",
    "        colour = [0, 204, 0]\n",
    "    else:\n",
    "        colour = [255, 0, 255]\n",
    "\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    r[mask == 1], g[mask == 1], b[mask == 1] = colour\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask, colour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction(pred, threshold, msk_threshold):\n",
    "    \"\"\"\n",
    "    :param pred: a dictionary of tensors with the predictions scores, masks and\n",
    "    detection boxes for one image\n",
    "    :param threshold: a float to threshold the probability score\n",
    "    :param msk_threshold: a float to threshold the segmentation masks\n",
    "    :return: numpy arrays of the thresholded masks, boxes and scores\n",
    "    \"\"\"\n",
    "\n",
    "    pred_score = list(pred['scores'].detach().cpu().numpy())\n",
    "    \n",
    "    # Get the score indices\n",
    "    pred_th = [pred_score.index(s) for s in pred_score if s > threshold][-1]\n",
    "    score = [s for s in pred_score if s > threshold]\n",
    "    # Threshold the masks and squeeze the batch dimmension\n",
    "    masks = (pred['masks']>msk_threshold).squeeze(1).detach().cpu().numpy()\n",
    "    pred_boxes = [[(int(i[0]), int(i[1])), (int(i[2]), int(i[3]))] \n",
    "                  for i in list(pred['boxes'].detach().cpu().numpy())]\n",
    "    masks = masks[:pred_th+1]\n",
    "   pred_boxes = pred_boxes[:pred_th+1]\n",
    "\n",
    "    return masks, pred_boxes, score\n",
    "\n",
    "def inst_segment(img, pred, threshold=0.6, mask_thr=0.6, rect_th=2, text_size=2, text_th=2,\n",
    "                 mask_out=True, colour='blue'):\n",
    "    \"\"\"\n",
    "    :param img: a numpy array with image\n",
    "    :param pred: a numpy array of the prediction\n",
    "    :param threshold: a float to threshold the detection boxes\n",
    "    :param mask_thr:  a float to threshold the mask predictions\n",
    "    :param rect_th:  a float of the boxes boundaries\n",
    "    :param text_size: a float or integer with the size of the font\n",
    "    :param text_th:  a float indicating the ticknes of the font\n",
    "    :param mask_out: a boolean  indicating wheteher only mask will be shown on the inference\n",
    "    :param colour: a string of the mask and boxes colour\n",
    "    :return: images with mask predictions\n",
    "    \"\"\"\n",
    "\n",
    "    masks, boxes, scores = get_prediction(pred, threshold, msk_threshold=mask_thr)\n",
    "\n",
    "    for i in range(len(masks)):\n",
    "        rgb_mask, rgb_colour = get_coloured_mask(masks[i], colour=colour)\n",
    "        # image, alpha, segmentation_map, beta, gamma)\n",
    "        img = cv2.addWeighted(np.array(img), 1, rgb_mask, 1, 0.2)\n",
    "        if not mask_out:\n",
    "            cv2.rectangle(img, boxes[i][0], boxes[i][1], color=rgb_colour, thickness=rect_th)\n",
    "            cv2.putText(img, str(np.round(scores[i],2)), boxes[i][0], cv2.FONT_HERSHEY_PLAIN,\n",
    "                      text_size, rgb_colour, thickness=text_th , lineType=cv2.LINE_AA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%def show_predictions(model, loader, threshold=0.2, msk_thres=0.2, mask_out=True,\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_predictions(model, loader, threshold=0.7, msk_thres=0.6, mask_out=True,\n",
    "                     outdir=None, fname=None, colour='blue'):\n",
    "    \"\"\"\n",
    "    :param model: model to evaluate\n",
    "    :param loader:  constructor with tensors of the test images and targets\n",
    "    :param threshold: float to select the boxes\n",
    "    :param msk_thres: float to threshold the masks\n",
    "    :param mask_out: boolean to show only mask or boxes and masks\n",
    "    :param outdir: str directory to save the inference\n",
    "    :param fname: str name to save the inference\n",
    "    :param colour: a string of the mask and boxes colour\n",
    "    :return: save and show predictions and probability scores on the test images\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    model = model\n",
    "    model.eval()\n",
    "\n",
    "    images, targets = next(iter(loader))\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "    for i, image in enumerate(images):\n",
    "        fig.add_subplot(len(images) // 4, 4, i + 1, xticks=[], yticks=[])\n",
    "        img = torchvision.transforms.ToPILImage()(image)\n",
    "        img = inst_segment(np.array(img), predictions[i], threshold, mask_thr=msk_thres,\n",
    "                           mask_out=mask_out,\n",
    "                           colour=colour)\n",
    "        img_id = targets[i]['image_id'].item()\n",
    "        name = covid.get_name(img_id)\n",
    "        plt.imshow(img)\n",
    "        plt.xlabel(name, color='blue', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if outdir is not None:\n",
    "        fname = f'predictions_{fname}.png'\n",
    "        return plt.savefig(os.path.join(outdir, fname), bbox_inches='tight',\n",
    "                            format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%show_predictions(model_.to('cpu'), test_loader, threshold=0.60, msk_thres=0.5, mask_out=True,\n"
    }
   },
   "outputs": [],
   "source": [
    "show_predictions(model_.to('cpu'), test_loader, threshold=0.70, msk_thres=0.6, mask_out=True,\n",
    "                 outdir=figs_dir, fname=model_name, colour='blue')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1wmCPq83lmweVA6uH27rxSXZ9Gw5Xa2mS",
     "timestamp": 1635762084027
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
